{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "D:\\Anaconda\\next_anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import gensim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "import scipy.stats\n",
    "import nltk\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "class WAE(nn.Module):\n",
    "    def __init__(self, encode_dims=[2000, 1024, 512, 20], decode_dims=[20, 1024, 2000], dropout=0.0, nonlin='relu'):\n",
    "        super(WAE, self).__init__()\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            f'enc_{i}': nn.Linear(encode_dims[i], encode_dims[i+1])\n",
    "            for i in range(len(encode_dims)-1)\n",
    "        })\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            f'dec_{i}': nn.Linear(decode_dims[i], decode_dims[i+1])\n",
    "            for i in range(len(decode_dims)-1)\n",
    "        })\n",
    "        self.latent_dim = encode_dims[-1]\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.nonlin = {'relu': F.relu, 'sigmoid': torch.sigmoid}[nonlin]\n",
    "        self.z_dim = encode_dims[-1]\n",
    "        \n",
    "    def encode(self, x):\n",
    "        hid = x\n",
    "        for i, (_,layer) in enumerate(self.encoder.items()):\n",
    "            hid = self.dropout(layer(hid))\n",
    "            if i < len(self.encoder)-1:\n",
    "                hid = self.nonlin(hid)\n",
    "        return hid\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            f'enc_{i}':nn.Linear(encode_dims[i],encode_dims[i+1]) \n",
    "            for i in range(len(encode_dims)-2)\n",
    "        })\n",
    "        self.fc_mu = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "        self.fc_logvar = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            f'dec_{i}':nn.Linear(decode_dims[i],decode_dims[i+1])\n",
    "            for i in range(len(decode_dims)-1)\n",
    "        })\n",
    "        self.latent_dim = encode_dims[-1]\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(encode_dims[-1],encode_dims[-1])\n",
    "        \n",
    "    def encode(self, x):\n",
    "        hid = x\n",
    "        for i,layer in self.encoder.items():\n",
    "            hid = F.relu(self.dropout(layer(hid)))\n",
    "        mu, log_var = self.fc_mu(hid), self.fc_logvar(hid)\n",
    "        return mu, log_var\n",
    "        \n",
    "class WTM:\n",
    "    def __init__(self, bow_dim=10000, n_topic=20, device=None, dist='gmm_std', taskname=None, dropout=0.0):\n",
    "        self.bow_dim = bow_dim\n",
    "        self.n_topic = n_topic\n",
    "        self.wae = WAE(encode_dims=[bow_dim, 1024, 512, n_topic], decode_dims=[n_topic, 512, bow_dim], dropout=dropout, nonlin='relu')\n",
    "        self.device = device\n",
    "        self.id2token = None\n",
    "        self.dist = dist\n",
    "        self.taskname = taskname\n",
    "        if device != None:\n",
    "            self.wae = self.wae.to(device)\n",
    "            \n",
    "    def inference(self, doc_bow):\n",
    "    # doc_bow: torch.tensor [vocab_size]; optional: np.array [vocab_size]\n",
    "        with torch.no_grad():\n",
    "            theta = F.softmax(self.wae.encode(doc_bow),dim=1)\n",
    "            return theta.detach().cpu().numpy().reshape(75,)\n",
    "        \n",
    "\n",
    "class GSM:\n",
    "    def __init__(self,bow_dim=10000,n_topic=20,taskname=None,device=None):\n",
    "        self.bow_dim = bow_dim\n",
    "        self.n_topic = n_topic\n",
    "        #TBD_fc1\n",
    "        self.vae = VAE(encode_dims=[bow_dim,1024,512,n_topic],decode_dims=[n_topic,512,bow_dim],dropout=0.0)\n",
    "        self.device = device\n",
    "        self.id2token = None\n",
    "        self.taskname = taskname\n",
    "        if device!=None:\n",
    "            self.vae = self.vae.to(device)\n",
    "\n",
    "    def inference(self,doc_bow):\n",
    "        # doc_bow: torch.tensor [vocab_size]; optional: np.array [vocab_size]\n",
    "        with torch.no_grad():\n",
    "            mu,log_var =self.vae.encode(doc_bow)\n",
    "            mu = self.vae.fc1(mu)\n",
    "            theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()   \n",
    "        \n",
    "        \n",
    "class LDA:\n",
    "    def __init__(self):\n",
    "        self.lda_model = gensim.models.ldamodel.LdaModel.load('D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\LDA_nips.ckpt')\n",
    "    \n",
    "    def inference(self,doc):\n",
    "        doc_bow = self.lda_model.id2word.doc2bow(doc)      #文档转换成bow\n",
    "        doc_lda = self.lda_model[doc_bow] \n",
    "        doc_tp = np.zeros(75)+0.001\n",
    "        for i in doc_lda:\n",
    "            doc_tp[i[0]] = i[1]\n",
    "        return doc_tp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAE(VAE):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0,emb_dim=300):\n",
    "        super(EVAE,self).__init__(encode_dims=encode_dims,decode_dims=decode_dims,dropout=dropout)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = encode_dims[0]\n",
    "        self.n_topic = encode_dims[-1]\n",
    "        self.rho = nn.Linear(emb_dim,self.vocab_size)\n",
    "        self.alpha = nn.Linear(emb_dim,self.n_topic)\n",
    "        self.decoder = None\n",
    "\n",
    "    def decode(self,z):\n",
    "        wght_dec = self.alpha(self.rho.weight) #[K,V]\n",
    "        beta = F.softmax(wght_dec,dim=0).transpose(1,0)\n",
    "        res = torch.mm(z,beta)\n",
    "        logits = torch.log(res+1e-6)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ETM:\n",
    "    def __init__(self,bow_dim=10000,n_topic=20,taskname=None,device=None,emb_dim=300):\n",
    "        self.bow_dim = bow_dim\n",
    "        self.n_topic = n_topic\n",
    "        self.emb_dim = emb_dim\n",
    "        #TBD_fc1\n",
    "        self.vae = EVAE(encode_dims=[bow_dim,1024,512,n_topic],decode_dims=[n_topic,512,bow_dim],dropout=0.0,emb_dim=emb_dim)\n",
    "        self.device = device\n",
    "        self.id2token = None\n",
    "        self.taskname = taskname\n",
    "        if device!=None:\n",
    "            self.vae = self.vae.to(device)\n",
    "            \n",
    "    def inference(self,doc_bow):\n",
    "    # doc_bow: torch.tensor [vocab_size]; optional: np.array [vocab_size]\n",
    "        if isinstance(doc_bow,np.ndarray):\n",
    "            doc_bow = torch.from_numpy(doc_bow)\n",
    "        doc_bow = doc_bow.reshape(-1,self.bow_dim).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            mu,log_var = self.vae.encode(doc_bow)\n",
    "            mu = self.vae.fc1(mu) \n",
    "            theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biterm():\n",
    "    wi = 0\n",
    "    wj = 0\n",
    "    z = 0\n",
    "\n",
    "    def __init__(self,w1=None,w2=None,s=None):\n",
    "        if w1 != None and w2 != None:\n",
    "            self.wi = min(w1,w2)\n",
    "            self.wj = max(w1,w2)\n",
    "        elif w1 == None and w2 == None and s != None:\n",
    "            w = s.split(' ')\n",
    "            self.wi = w[0]\n",
    "            self.wj = w[1]\n",
    "            self.z = w[2]\n",
    "\n",
    "    def get_wi(self):\n",
    "        return self.wi\n",
    "\n",
    "    def get_wj(self):\n",
    "        return self.wj\n",
    "\n",
    "    def get_z(self):\n",
    "        return self.z\n",
    "\n",
    "    def set_z(self,k):\n",
    "        self.z = k\n",
    "\n",
    "    def reset_z(self):\n",
    "        self.z = -1\n",
    "\n",
    "    def str(self):\n",
    "        _str = \"\"\n",
    "        _str += str(self.wi) + '\\t' + str(self.wj) + '\\t\\t' + str(self.z)\n",
    "        return _str\n",
    "class Doc():\n",
    "    '''\n",
    "    @description: 处理文本的类\n",
    "    @param {type} \n",
    "    @return: \n",
    "    '''\n",
    "    ws = []\n",
    "\n",
    "    def __init__(self,s,voc):\n",
    "        self.ws = []\n",
    "        self.dict = dict()\n",
    "        with open(voc, 'r') as f:\n",
    "            voclist = f.read().splitlines()\n",
    "        for i in range(len(voclist)):\n",
    "            self.dict[voclist[i]] = i\n",
    "        self.read_doc(s)\n",
    "\n",
    "    def read_doc(self,s):\n",
    "        if s != '\\n':\n",
    "            for w in s.split(' '):\n",
    "                if w not in self.dict:\n",
    "                    continue\n",
    "                self.ws.append(int(self.dict[w]))\n",
    "                \n",
    "    def show(self):\n",
    "        print('ceshi:', self.ws)\n",
    "        print('字典:', self.dict)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.ws)\n",
    "\n",
    "    def get_w(self,i):\n",
    "        assert(i<len(self.ws))\n",
    "        return self.ws[i]\n",
    "\n",
    "    ''' \n",
    "      Extract biterm from a document\n",
    "        'win': window size for biterm extraction\n",
    "        'bs': the output biterms\n",
    "    '''\n",
    "    def gen_biterms(self,bs,win=15):\n",
    "        if(len(self.ws)<2):\n",
    "            return\n",
    "        for i in range(len(self.ws)-1):\n",
    "            for j in range(i+1,min(i+win,len(self.ws))):\n",
    "                bs.append(Biterm(self.ws[i],self.ws[j]))\n",
    "                \n",
    "class BTModel:\n",
    "    def __init__(self, model_dir, voca_path):\n",
    "        self.zw_pt = list()\n",
    "        with open(model_dir+'k75.pw_z') as f:\n",
    "            for line in f.readlines():\n",
    "                self.zw_pt.append([float(p) for p in line.split()])\n",
    "        self.pz = [float(p) for p in open(model_dir+'k75.pz').readline().split()]\n",
    "        self.voca_path = voca_path\n",
    "        \n",
    "    def inference(self, test_path):\n",
    "        def softmax(x):\n",
    "            exp_x = np.exp(x)\n",
    "            sum_exp_x = np.sum(exp_x)\n",
    "            y = exp_x/sum_exp_x\n",
    "            return y\n",
    "        \n",
    "        def load_docs(docs_pt):\n",
    "            bs = []\n",
    "#             print(\"load docs: \" + docs_pt)\n",
    "#             rf = open(docs_pt)\n",
    "#             if not rf:\n",
    "#                 print(\"file not found: \" + docs_pt)\n",
    "#             for line in rf.readlines():\n",
    "            line = docs_pt\n",
    "            d = Doc(line, self.voca_path)\n",
    "#             d.show()\n",
    "#             print('d in load_docs:', d)\n",
    "            biterms = []\n",
    "            d.gen_biterms(biterms)\n",
    "            for b in biterms:\n",
    "                bs.append(b)\n",
    "            # print(len(bs))\n",
    "            return bs\n",
    "        bs = load_docs(test_path)\n",
    "        K = len(self.pz)\n",
    "        #print('KKKK', K)\n",
    "        zw_pt = self.zw_pt\n",
    "        pz_d = [0.0]*K\n",
    "        if len(bs) == 1:\n",
    "            for k in range(K):\n",
    "                pz_d[k] = math.log(self.pz[k]) + math.log(zw_pt[k][bs[0].get_wi()])\n",
    "            pz_d = softmax(pz_d)\n",
    "        else:\n",
    "            for bs_i in bs:\n",
    "                w1 = bs_i.get_wi()\n",
    "                w2 = bs_i.get_wj()\n",
    "                pz_b = [0.0]*K\n",
    "                for k in range(K):\n",
    "    #                 print(pz[k])\n",
    "                    pz_b[k] = math.log(self.pz[k]) + math.log(zw_pt[k][w1]) + math.log(zw_pt[k][w2])\n",
    "    #             print(pz_b)\n",
    "                pz_b = softmax(pz_b)\n",
    "                for kk in range(K):\n",
    "                    pz_d[kk] += pz_b[kk]\n",
    "            pz_d = softmax(pz_d)\n",
    "        return pz_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTModel:\n",
    "    def __init__(self, path):\n",
    "        # path : 模型的存储路径 model-50\n",
    "        self.model = pickle.load(open(path, \"rb\"))\n",
    "    def inference(self, text):\n",
    "        # text: str\n",
    "        log_likelihood, lambda_values, nu_square_values = self.model.inference([text])\n",
    "        def softmax(x):\n",
    "            exp_x = np.exp(x)\n",
    "            sum_exp_x = np.sum(exp_x)\n",
    "            y = exp_x/sum_exp_x\n",
    "            return y\n",
    "        return softmax(lambda_values)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT and DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp # 载入库\n",
    "\n",
    "class PTModel:\n",
    "    def __init__(self, path):\n",
    "        # path : 模型的存储路径\n",
    "        self.model = tp.PTModel.load(path)\n",
    "    def inference(self, text):\n",
    "        # text: 以空格间隔开的文本\n",
    "        #txt = text.split(' ')\n",
    "        doc_inst = self.model.make_doc(text)\n",
    "        topic_dist, ll = self.model.infer(doc_inst)\n",
    "        return topic_dist\n",
    "class DMRModel:\n",
    "    def __init__(self, path):\n",
    "        # path : 模型的存储路径\n",
    "        self.model = tp.DMRModel.load(path)\n",
    "    def inference(self, text):\n",
    "        # text: 以空格间隔开的文本\n",
    "        #txt = text.split(' ')\n",
    "        doc_inst = self.model.make_doc(text)\n",
    "        topic_dist, ll = self.model.infer(doc_inst)\n",
    "        return topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同义词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "we = pd.read_csv(\"D:\\毕设\\ML算法实现\\主题模型攻击\\pre_processing.csv\")#相似词\n",
    "we['similar_words'] = we['similar_words'].map(lambda x: eval(x))\n",
    "def evals(x):\n",
    "    if x == '<class \\'list\\'>':\n",
    "        return []\n",
    "    else:\n",
    "        return eval(x)\n",
    "we['similar_mixed'] = we['similar_mixed'].map(lambda x: evals(x))\n",
    "\n",
    "# 索引字典\n",
    "word2id = {}\n",
    "count = 0\n",
    "for i in we['word']:\n",
    "    word2id[i] = count \n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bow(data, min_length):\n",
    "    \"\"\"Convert index lists to bag of words representation of documents.\"\"\"\n",
    "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
    "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
    "    return np.array(vect)\n",
    "\n",
    "def Tokenizer(sent,stopwords=None):\n",
    "        # Tokenizer for English. \n",
    "        pat = re.compile(r'[0-9!\"#$%&\\'()*+,-./:;<=>?@—，。：★、￥…【】（）《》？“”‘’！\\[\\\\\\]^_`{|}~\\u3000]+')\n",
    "        tokens = [re.sub(pat,r'',t).strip() for t in sent.split(' ')]\n",
    "        tokens = [t for t in tokens if t!='' and len(t)>1]\n",
    "        tokens = [t.lower() for t in tokens]    \n",
    "        if stopwords is not None:\n",
    "            tokens = [t for t in tokens if not (t in stopwords)]                                               \n",
    "        return tokens\n",
    "\n",
    "def word_embedding(dictionary,vim_txt,stopwords=None):\n",
    "    '''\n",
    "    #input 字典，文本txt\n",
    "    #output 计数向量tensor 单词索引list\n",
    "    '''\n",
    "    with open(vim_txt,'r',encoding='utf-8') as f:\n",
    "        txt = f.read()\n",
    "    ori_token = Tokenizer(txt,stopwords)\n",
    "    ori_bow = torch.zeros(1,len(dictionary))\n",
    "    ori_index = []\n",
    "    new_token = []\n",
    "    for token in ori_token:\n",
    "            try:\n",
    "                ori_idx = dictionary.token2id[token.lower()]\n",
    "                ori_index.append(ori_idx)\n",
    "                ori_bow[0][ori_idx] += 1\n",
    "                new_token.append(token)\n",
    "            except:\n",
    "                continue\n",
    "    return ori_bow,ori_index,new_token,len(ori_token)\n",
    "\n",
    "def topk_import_words(ori_index,ori_bow,ori_token,model,dictionary,ori_result,model_name,i=0):\n",
    "    '''\n",
    "    当model_name = LDA时，ori_index = token，ori_bow\n",
    "    #关键词排序\n",
    "    #input\n",
    "    #topk：前k个词\n",
    "    #ori_index:单词索引\n",
    "    #ori_bow:单词向量\n",
    "    #model：攻击模型\n",
    "    #ori_result:原始主题分布\n",
    "    #output\n",
    "    #topk单词字典（word：KL value）\n",
    "    #topk单词索引\n",
    "    '''\n",
    "    diff_re ={} # 删除词后的主题分布\n",
    "    if model_name == 'LDA' or model_name == \"PT\" or model_name == \"DMR\":\n",
    "        for word in set(ori_token):\n",
    "            temp = copy.deepcopy(ori_token)\n",
    "            temp.remove(word)\n",
    "            LDA_tmp_result = model.inference(temp)\n",
    "            try:\n",
    "                diff_re[word] = scipy.stats.entropy(LDA_tmp_result,ori_result)\n",
    "            except:\n",
    "                print(len(LDA_tmp_result),len(ori_result))\n",
    "                print(temp)\n",
    "                 \n",
    "    elif model_name == 'CTM' or model_name == 'BTM':\n",
    "        for word in set(ori_token):\n",
    "            temp = copy.deepcopy(ori_token)\n",
    "            temp.remove(word)\n",
    "            tmp = \" \".join(temp)\n",
    "            LDA_tmp_result = model.inference(tmp)\n",
    "            diff_re[word] = scipy.stats.entropy(LDA_tmp_result,ori_result)\n",
    "            #print(LDA_tmp_result)\n",
    "            \n",
    "    else:\n",
    "        for idx in ori_index:\n",
    "            tmp_bow = ori_bow.clone()\n",
    "            tmp_bow[i][idx] = 0\n",
    "            tmp_result = model.inference(tmp_bow)\n",
    "            diff_re[dictionary[idx]] = scipy.stats.entropy(tmp_result,ori_result)\n",
    "\n",
    "    #return sorted(diff_re.items(),key = lambda item:item[1],reverse = True)\n",
    "    \n",
    "    return diff_re\n",
    "\n",
    "def softmax(kl):\n",
    "    s = sum(kl.values())\n",
    "    for i in kl.keys():\n",
    "        kl[i] = kl[i]/s\n",
    "    #return sorted(kl.items(),key = lambda item:item[1],reverse = True)\n",
    "    return kl\n",
    "\n",
    "def similar_word(t_word):\n",
    "    #返回该词的近义词列表\n",
    "    return we['similar_words'][word2id[t_word]:word2id[t_word]+1][word2id[t_word]]\n",
    "\n",
    "def rep_sim_word(ori_bow,ori_token,sim_word,t_word,dictionary,model_name,model,i=0):\n",
    "    '''\n",
    "    返回kl散度\n",
    "    '''\n",
    "    #修改词向量\n",
    "    token = copy.deepcopy(ori_token)\n",
    "    bow = ori_bow.clone()\n",
    "    if model_name == \"LDA\" or model_name == \"PT\" or model_name == \"DMR\":\n",
    "        ori = model.inference(token)\n",
    "        token[token.index(t_word)] = sim_word\n",
    "        tmp_re = model.inference(token)\n",
    "        return scipy.stats.entropy(tmp_re,ori)\n",
    "    \n",
    "    elif model_name == 'CTM' or model_name == 'BTM':\n",
    "        tmp = \" \".join(token)\n",
    "        ori = model.inference(tmp)\n",
    "        token[token.index(t_word)] = sim_word\n",
    "        tmp = \" \".join(token)\n",
    "        tmp_re = model.inference(tmp)\n",
    "        return scipy.stats.entropy(tmp_re,ori)\n",
    "        \n",
    "    else:\n",
    "        ori = model.inference(bow)\n",
    "        tmp = bow[i][dictionary.token2id[t_word]]\n",
    "        bow[i][dictionary.token2id[sim_word]] += tmp\n",
    "        bow[i][dictionary.token2id[t_word]] = 0\n",
    "        tmp_re = model.inference(bow)\n",
    "        return scipy.stats.entropy(tmp_re,ori)\n",
    "\n",
    "def sim_word_kl(sim_word_list,ori_bow,ori_token,threshold,weight,i=0):\n",
    "    sim_kl = {}\n",
    "    for j in range(len(sim_word_list)):\n",
    "        s_ = 0 #同义词的kl散度变化\n",
    "        sim_word = sim_word_list[j][0]\n",
    "        try:\n",
    "            a = dictionary.token2id[sim_word]\n",
    "        except:\n",
    "            #print(f'{sim_word} 该同义词不在字典中')\n",
    "            continue\n",
    "        \n",
    "        for _ in range(model_num):\n",
    "            tmp =  rep_sim_word(ori_bow,ori_token,sim_word,word,dictionary,model_name[_],model_set[_],i=0)\n",
    "            s_ += tmp/threshold[_] * weight[_]\n",
    "        sim_kl[sim_word] = s_\n",
    "    return sorted(sim_kl.items(),key = lambda item:item[1],reverse = True)\n",
    "\n",
    "def generate_vim(ori_txt,vim_txt,pair):\n",
    "    with open(ori_txt,'r') as f:\n",
    "        ori = f.read()\n",
    "    ori = Tokenizer(ori)\n",
    "    for i in pair.keys():\n",
    "        while i in ori:\n",
    "            ori[ori.index(i)] = pair[i]\n",
    "    txt = \" \".join(ori)\n",
    "    with open(vim_txt,'w') as f:\n",
    "        f.write(txt)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load('D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\\\nips_16157.dict')\n",
    "WTM_D_path = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\WTM_D.pickle'\n",
    "WTM_parameters = torch.load(WTM_D_path,map_location=torch.device('cpu'))\n",
    "WTM_D_model = WTM(bow_dim = 16157,n_topic= 75)\n",
    "WTM_D_model.wae.load_state_dict(WTM_parameters)\n",
    "\n",
    "WTM_G_path = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\WTM_G.pickle'\n",
    "WTM_parameters = torch.load(WTM_G_path,map_location=torch.device('cpu'))\n",
    "WTM_G_model = WTM(bow_dim = 16157,n_topic= 75)\n",
    "WTM_G_model.wae.load_state_dict(WTM_parameters)\n",
    "\n",
    "GSM_path = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\GSM_nips.pickle'\n",
    "GSM_parameters = torch.load(GSM_path,map_location=torch.device('cpu'))\n",
    "GSM_model = GSM(bow_dim = 16157,n_topic= 75)# 参数设置\n",
    "GSM_model.vae.load_state_dict(GSM_parameters)\n",
    "LDA_model = LDA()\n",
    "ETM_path = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\ETM_nips.pickle'\n",
    "ETM_parameters = torch.load(ETM_path,map_location=torch.device('cpu'))\n",
    "ETM_model = ETM(bow_dim = 16157,n_topic= 75)# 参数设置\n",
    "ETM_model.vae.load_state_dict(ETM_parameters)\n",
    "\n",
    "DMR_model = DMRModel('NIPS_DMRModel.bin')\n",
    "PT_model = PTModel('NIPS_PTModel.bin')\n",
    "\n",
    "CTM_model = CTModel('D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\ctm\\\\nips_model-50')\n",
    "#BTM_model = BTModel('model/', 'new_voca.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理第0个文本\n",
      "处理第1个文本\n",
      "处理第2个文本\n",
      "处理第3个文本\n",
      "处理第4个文本\n",
      "处理第5个文本\n",
      "处理第6个文本\n",
      "处理第7个文本\n",
      "处理第8个文本\n",
      "处理第9个文本\n",
      "处理第10个文本\n",
      "处理第11个文本\n",
      "处理第12个文本\n",
      "处理第13个文本\n",
      "处理第14个文本\n",
      "处理第15个文本\n",
      "处理第16个文本\n",
      "处理第17个文本\n",
      "处理第18个文本\n",
      "处理第19个文本\n",
      "处理第20个文本\n",
      "处理第21个文本\n",
      "处理第22个文本\n",
      "处理第23个文本\n",
      "处理第24个文本\n",
      "处理第25个文本\n",
      "处理第26个文本\n",
      "处理第27个文本\n",
      "处理第28个文本\n",
      "处理第29个文本\n",
      "处理第30个文本\n",
      "处理第31个文本\n",
      "处理第32个文本\n",
      "处理第33个文本\n",
      "处理第34个文本\n",
      "处理第35个文本\n",
      "处理第36个文本\n",
      "处理第37个文本\n",
      "处理第38个文本\n",
      "处理第39个文本\n",
      "处理第40个文本\n",
      "处理第41个文本\n",
      "处理第42个文本\n",
      "处理第43个文本\n",
      "处理第44个文本\n",
      "处理第45个文本\n",
      "处理第46个文本\n",
      "处理第47个文本\n",
      "处理第48个文本\n",
      "处理第49个文本\n"
     ]
    }
   ],
   "source": [
    "lda = []\n",
    "rate = []\n",
    "topk = 0.05\n",
    "a_ = [\"GSM\"]\n",
    "b_ = [GSM_model]\n",
    "for aa in range(len(a_)):\n",
    "    for i in range(50):\n",
    "        print(\"处理第{}个文本\".format(i))\n",
    "        model_name = [a_[aa]]\n",
    "        vim_txt = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\\\nips_' + str(i) + '.txt'\n",
    "        new_vim_txt ='D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\\\nips_based\\\\'+model_name[0] + \"based2_\" + str(topk)+ \"_\" + str(i) + '.txt'\n",
    "\n",
    "        model_set = [b_[aa]]\n",
    "        model_num = len(model_name)\n",
    "        r_set = []\n",
    "        ori_bow,ori_index,ori_token,length = word_embedding(dictionary,vim_txt,stopwords = stopwords)\n",
    "        for j in range(model_num):\n",
    "            if model_name[j] == \"LDA\" or model_name[j] == \"PT\" or model_name[j] == \"DMR\":\n",
    "                r = model_set[j].inference(ori_token)\n",
    "                r_set.append(r)\n",
    "            elif model_name[j] == 'CTM' or model_name[j] == 'BTM' :\n",
    "                tmp = \" \".join(ori_token)\n",
    "                r = model_set[j].inference(tmp)\n",
    "                r_set.append(r)\n",
    "            else:\n",
    "                r = model_set[j].inference(ori_bow)\n",
    "                r_set.append(r)\n",
    "\n",
    "\n",
    "        #ori_gsm_re = LDA_model.inference(ori_token)\n",
    "        # 计算每个词的kl散度\n",
    "        #kl_set = []\n",
    "        #for j in range(model_num):\n",
    "        #    a = topk_import_words(ori_index,ori_bow,ori_token,model_set[j],dictionary,r_set[j],model_name[j])\n",
    "        #    #print(a)\n",
    "        #    kl_set.append(a)\n",
    "        ## softmax\n",
    "        #soft_kl_set = []\n",
    "        #for j in range(model_num):\n",
    "        #    a = softmax(kl_set[j])\n",
    "        #    soft_kl_set.append(a)\n",
    "        ##softmax相加\n",
    "        #\n",
    "        #new_kl = {}\n",
    "        #for jj in soft_kl_set[0].keys():\n",
    "        #    s_kl = 0\n",
    "        #    for j in range(model_num):\n",
    "        #        s_kl += soft_kl_set[j][jj]\n",
    "        #    new_kl[jj] = s_kl\n",
    "        #\n",
    "\n",
    "        ###############################################################\n",
    "        threshold =np.array([1.57])\n",
    "        #hyp = np.array([1,38,1])\n",
    "        pair = {}\n",
    "        process = []\n",
    "        weight = np.ones(model_num)\n",
    "        weight_set = []\n",
    "        arrive = np.zeros(model_num)#到达阈值时截断\n",
    "\n",
    "        #print(new_kl)\n",
    "        #new_kl =  sorted(new_kl.items(),key = lambda item:item[1],reverse = True)\n",
    "        count = 0\n",
    "        #LDA_re = 0\n",
    "        rand_set = []\n",
    "        while (len(pair) <= int(len(ori_token) * topk)):\n",
    "            rand_int = np.random.randint(len(ori_token))\n",
    "            if rand_int not in rand_set:\n",
    "                rand_set.append(rand_int)\n",
    "                word = ori_token[rand_int]\n",
    "                try:\n",
    "                    sim_word_list = similar_word(word)\n",
    "                except:\n",
    "                    #print(f'{word} 该词中无近义词')\n",
    "                    continue\n",
    "\n",
    "                sim_kl = sim_word_kl(sim_word_list,ori_bow,ori_token,threshold,weight)\n",
    "                #print(sim_kl)\n",
    "                if len(sim_kl) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    #tmp = np.random.randint(len(sim_kl))\n",
    "                    #best_sim_word = sim_kl[tmp][0]\n",
    "                    best_sim_word = sim_kl[0][0]\n",
    "                    pair[word] = best_sim_word\n",
    "\n",
    "                #对oir_bow和ori_token迭代\n",
    "                tmp = ori_bow[0][dictionary.token2id[word]]\n",
    "                ori_bow[0][dictionary.token2id[best_sim_word]] += tmp\n",
    "                ori_bow[0][dictionary.token2id[word]] = 0\n",
    "                ori_token[ori_token.index(word)] = best_sim_word\n",
    "\n",
    "                if len(pair) > int(len(ori_token) * topk):\n",
    "                    #print(len(pair),length *topk)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "        generate_vim(vim_txt,new_vim_txt,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'devising': 'assembling',\n",
       " 'foreign': 'macroeconomic',\n",
       " 'blind': 'myopic',\n",
       " 'intervals': 'alternations',\n",
       " 'redundancy': 'obviates',\n",
       " 'week': 'season',\n",
       " 'active': 'dormant',\n",
       " 'minimizing': 'lowering',\n",
       " 'colorado': 'mountain',\n",
       " 'advised': 'requested',\n",
       " 'drift': 'velocity',\n",
       " 'truncating': 'truncated',\n",
       " 'cost': 'infeasibility',\n",
       " 'core': 'conforms',\n",
       " 'separated': 'separating',\n",
       " 'scaling': 'resizing',\n",
       " 'memorizing': 'annotating',\n",
       " 'equivalent': 'translates',\n",
       " 'depicts': 'revolves',\n",
       " 'coefficients': 'subspaces',\n",
       " 'mutually': 'socially',\n",
       " 'search': 'returns',\n",
       " 'days': 'occasions',\n",
       " 'feature': 'preview',\n",
       " 'features': 'screens',\n",
       " 'stream': 'flow',\n",
       " 'incurs': 'forbids',\n",
       " 'suppose': 'supposes',\n",
       " 'collect': 'money',\n",
       " 'casting': 'annealing',\n",
       " 'obtaining': 'earning',\n",
       " 'exponentially': 'drastically',\n",
       " 'generalized': 'theorems',\n",
       " 'pick': 'selects',\n",
       " 'complexity': 'indeterminacy',\n",
       " 'exponential': 'factorial',\n",
       " 'pools': 'buckets',\n",
       " 'achieving': 'surpassing',\n",
       " 'reducing': 'mitigating',\n",
       " 'reduce': 'undesired',\n",
       " 'obtains': 'discards',\n",
       " 'true': 'provable',\n",
       " 'adaptive': 'operant',\n",
       " 'falls': 'locks',\n",
       " 'drawn': 'drawings',\n",
       " 'element': 'dissimilarity',\n",
       " 'expected': 'guaranteed',\n",
       " 'label': 'sampler',\n",
       " 'pool': 'bucket',\n",
       " 'goal': 'assists',\n",
       " 'existing': 'precluding',\n",
       " 'sampled': 'textures',\n",
       " 'noisy': 'crowds',\n",
       " 'answer': 'caveats',\n",
       " 'determine': 'delineate',\n",
       " 'bounds': 'multiplicities',\n",
       " 'descent': 'occupation',\n",
       " 'curvature': 'velocities',\n",
       " 'prescribe': 'prescriptions',\n",
       " 'prove': 'doubt',\n",
       " 'gradient': 'temperature',\n",
       " 'advice': 'prescriptions',\n",
       " 'administered': 'regulated',\n",
       " 'contingent': 'reinforcements',\n",
       " 'ascent': 'summit',\n",
       " 'absolutely': 'demonstrably',\n",
       " 'neurons': 'dopamine',\n",
       " 'guidance': 'guidelines',\n",
       " 'bump': 'bone',\n",
       " 'language': 'phonemes',\n",
       " 'leaky': 'pump',\n",
       " 'habituation': 'amygdala',\n",
       " 'shortest': 'bisection',\n",
       " 'strengthen': 'reinforce',\n",
       " 'rises': 'descends',\n",
       " 'appearance': 'resemblance',\n",
       " 'sketches': 'illustrations',\n",
       " 'cascaded': 'interleaving'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(ori_txt,vim_txt,model,model_name):\n",
    "    # model:[LDA_model,GSM_model]模型列表\n",
    "    ori_bow,ori_index,ori_token,o_length = word_embedding(dictionary,ori_txt)\n",
    "    vim_bow,vim_index,vim_token,v_length = word_embedding(dictionary,vim_txt)\n",
    "    ori_r_set = []\n",
    "    vim_r_set = []\n",
    "    for i in range(len(model)):\n",
    "        if model_name[i] == \"LDA\" or model_name[i] == \"PT\" or model_name[i] == \"DMR\":\n",
    "            r = model[i].inference(ori_token)\n",
    "            ori_r_set.append(r)\n",
    "            r = model[i].inference(vim_token)\n",
    "            vim_r_set.append(r)\n",
    "        elif model_name[i] == 'CTM' or model_name[i] == 'BTM':\n",
    "            tmp = \" \".join(ori_token)\n",
    "            r = model[i].inference(tmp)\n",
    "            ori_r_set.append(r)\n",
    "            temp = \" \".join(vim_token)\n",
    "            r = model[i].inference(temp)\n",
    "            vim_r_set.append(r)\n",
    "        else:\n",
    "            r = model[i].inference(ori_bow)\n",
    "            ori_r_set.append(r)\n",
    "            r = model[i].inference(vim_bow)\n",
    "            vim_r_set.append(r)\n",
    "            \n",
    "    kl =[]\n",
    "    for i in range(len(model)):\n",
    "        kl.append(scipy.stats.entropy(ori_r_set[i],vim_r_set[i]))\n",
    "    return kl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 1.31948030e-30, 1.39091559e-28, 4.39655862e-26,\n",
       "       1.34543214e-28, 1.05980160e-29, 2.60137912e-30, 6.83098688e-30,\n",
       "       2.03647299e-30, 4.06219087e-28, 1.89323505e-29, 2.07297345e-26,\n",
       "       2.12961961e-29, 3.16446104e-30, 1.53673980e-29, 1.87752610e-30,\n",
       "       3.89627685e-28, 2.77583626e-30, 5.03918221e-29, 2.53041710e-24,\n",
       "       2.85976020e-29, 3.56920866e-29, 1.82790486e-29, 6.91914232e-29,\n",
       "       9.59938694e-29, 5.55866337e-25, 3.19525796e-29, 7.79080458e-29,\n",
       "       2.31229581e-28, 3.67571827e-30, 9.29179137e-29, 3.86517571e-29,\n",
       "       5.24011127e-28, 1.74567741e-28, 1.03388552e-27, 1.91194299e-30,\n",
       "       7.72147667e-30, 8.77749298e-28, 2.26624172e-29, 1.38473749e-30,\n",
       "       2.08135380e-29, 9.06694949e-29, 1.03153155e-30, 6.06971783e-28,\n",
       "       3.04144989e-28, 2.56566095e-30, 6.62209126e-30, 5.79122597e-30,\n",
       "       7.97196398e-28, 2.15451891e-30, 1.38263380e-30, 3.26161474e-30,\n",
       "       8.52770626e-30, 2.44950062e-27, 1.10284960e-28, 9.86128804e-27,\n",
       "       2.05627866e-30, 3.52593642e-28, 1.50565510e-28, 1.77372138e-28,\n",
       "       1.49031883e-28, 2.07201143e-28, 9.23818223e-29, 5.74249399e-30,\n",
       "       1.18859699e-29, 2.02793093e-29, 1.32539734e-29, 1.64977540e-28,\n",
       "       1.94091956e-25, 4.91488051e-29, 2.47691152e-28, 3.76240901e-29,\n",
       "       3.59130677e-28, 2.47589488e-29, 2.65894721e-31])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_bow,ori_index,ori_token,length = word_embedding(dictionary,\"BTMtrans_0.05_0.txt\",stopwords = stopwords)\n",
    "tmp = \" \".join(ori_token)\n",
    "r = BTM_model.inference(tmp)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_name = [\"LDA\",\"WTM_G\",\"GSM\",\"WTM_D\",\"ETM\",\"PT\",\"DMR\"]\n",
    "#eval_model_name = [\"CTM\",\"BTM\"]\n",
    "eval_model = [LDA_model,WTM_G_model,GSM_model,WTM_D_model,ETM_model,PT_model,DMR_model]\n",
    "#eval_model = [CTM_model,BTM_model]\n",
    "topk = 0.05\n",
    "kl = []\n",
    "for i in range(50):\n",
    "    if i == 79:\n",
    "        continue\n",
    "    ori_txt = 'D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\\\nips_' + str(i) + '.txt'\n",
    "    vim_txt ='D:\\毕设\\ML算法实现\\主题模型攻击\\\\nips_model\\\\nips_based\\\\'+eval_model_name[5] + \"based2_\" + str(topk)+ \"_\" + str(i) + '.txt'\n",
    "    #vim_txt =eval_model_name[4] + \"trans_\" + str(topk)+ \"_\" + str(i) + '.txt'\n",
    "    #vim_txt =eval_model_name[8] + \"trans_\" + str(topk)+ \"_\" + str(i) + '.txt'\n",
    "    tmp = eval(ori_txt,vim_txt,eval_model,eval_model_name)\n",
    "    kl.append(tmp)\n",
    "kl = np.array(kl)\n",
    "#print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:0.5266752925776905\n",
      "WTM_G:0.4973141551017761\n",
      "GSM:0.4097999185323715\n",
      "WTM_D:0.6294758319854736\n",
      "ETM:0.5813735127449036\n",
      "PT:0.06602989509701729\n",
      "DMR:0.6373244524002075\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eval_model_name)):\n",
    "    print(\"{}:{}\".format(eval_model_name[i],np.median(kl[:,i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NIPSbased2迁移性 | LDA | WTM-G | GSM | WTM-D | AVITM |ETM |CTM | BTM | PTM |DMR|ATI\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----|----\n",
    "|LDA | 0.8 | 0.78 | 0.52 | 0.74 | 0.56| 0.63|0.15|-|0.06|0.8|0.56\n",
    "|WTM-G | 0.45 | 0.63 | 0.44 | 0.54 | 0.51|  0.38|0.17|-|0.06|0.54|0.4\n",
    "|GSM | 0.48 | 0.61 | 0.62 | 0.73| 0.61| 0.44|0.16|-|0.05|0.64|0.48\n",
    "| WTM-D | 0.48| 0.58 | 0.46 | 0.85 | 0.61| 0.41|0.17|-|0.06|0.5|0.45\n",
    "| AVITM | 0.46 | 0.51| 0.42 | 0.61 | 0.8 |0.42|0.12|-|0.06|0.5|0.43\n",
    "| ETM | 0.52 | 0.49 | 0.4 | 0.62 | 0.62| 0.59|0.17|-|0.06|0.63|0.45\n",
    "| CTM | 0.53|0.59 |0.32|0.55|0.58|0.38|0.14|-|0.04|0.56|0.41\n",
    "| BTM | | | \n",
    "| PTM |0.69| 0.76| 0.46|0.84|0.61|0.53|0.15|-|0.1|0.7|0.53\n",
    "| DMR |0.56|0.63 |0.41|0.71|0.62|0.42|0.15|-|0.06|0.61|0.46\n",
    "|AT| 0.55 |0.62|0.45|0.69|0.61|0.47|0.15|-|0.06|0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NIPSbased迁移性 | LDA | WTM-G | GSM | WTM-D | AVITM |ETM |CTM | BTM | PTM |DMR\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----\n",
    "|LDA | 0.61 | 0.63 | 0.44 | 0.72 | -| 0.53|0.15|-|0.05|0.61\n",
    "|WTM-G | 0.6 | 0.69 | 0.43 | 0.55 | -|  0.49|0.17|-|0.05|0.7\n",
    "|GSM | 0.56 | 0.62 | 0.45 | 0.73| -| 0.54|0.16|-|0.05|0.64\n",
    "| WTM-D | 0.61| 0.62 | 0.46 | 0.67 | -| 0.54|0.17|-|0.06|0.7\n",
    "| AVITM | - | - | - | - | - |-\n",
    "| ETM | 0.59 | 0.72 | 0.38 | 0.84 | -| 0.59|0.17|-|0.05|0.73\n",
    "| CTM | 0.53|0.59 |0.32|0.55|-|0.38|0.14|-|0.04|0.56\n",
    "| BTM | | | \n",
    "| PTM |0.59| 0.56| 0.4|0.78|-|0.61|0.15|-|0.05|0.6\n",
    "| DMR |0.6|0.7 |0.41|0.71|-|0.5|0.15|-|0.05|0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |迁移性 | LDA | WTM-G | GSM | WTM-D | AVITM |ETM |CTM | BTM | PTM |DMR | 迁移强度\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----\n",
    "|LDA | 0.41 | 0.015 | 0.005 | 0.017 | 0.87| 0 |  0.01| 0 | 0.09 | 0.36 |0.17\n",
    "|WTM-G | 0.582 | 0.47 | 0.1 | 0.266 | 0.88|  0.12 | 0.1 | 0 | 0.18 | 0.6 | 0.32\n",
    "|GSM | 0.52 | 0.24 | 0.26 | 0.23 | 0.879| 0.24 | 0.09| 0 |   0.18 | 0.54 | 0.31\n",
    "| WTM-D | 0.58| 0.28 | 0.08 | 0.5 | 0.88| 0.08 | 0.09 | 0 |  0.19 | 0.55 | 0.32\n",
    "| AVITM | 0.24 | 0.02 | 0.008 | 0.02 | 0.31 |0.008 | 0.1 |0  | 0.11 | 0.3 | 0.11\n",
    "| ETM | 0.5 | 0.2 | 0.16 | 0.2 | 0.8| 0.67 | 0.09 |0 |  0.17 | 0.56 | 0.33\n",
    "| CTM | 0.26|0.015 | 0.006 | 0.015 | 0.87 | 0.006 |0.03 | 0 | 0.09 | 0.35 |0.16\n",
    "| BTM | 0.26| 0.02 | 0.006 | 0.018 |0.89 |0.09 | 0.015 | 0 | 0.1 | 0.36 | 0.17\n",
    "| PTM | 0.246| 0.02|0.007|0.02| 0.87| 0.006 | 0.01|0 | 0.15 |0.37 | 0.16\n",
    "| DMR |0.226 |0.02 |0.007 | 0.02 |   0.89|  0.008 | 0.01 | 0| 0.12 | 0.41 |0.17\n",
    "|平均迁移性(阈值) | 0.38 | 0.13 | 0.06 | 0.13 | 0.8 | 0.12 | 0.05 | 0 | 0.13 | 0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_txt = \"ori_1.txt\"\n",
    "vim_txt = \"AVITMtrans_0.05_1.txt\"\n",
    "ori_bow,ori_index,ori_token,o_length = word_embedding(dictionary,ori_txt)\n",
    "vim_bow,vim_index,vim_token,v_length = word_embedding(dictionary,vim_txt)\n",
    "for i in range(len(ori_index)):\n",
    "    if ori_index[i] != vim_index[i]:\n",
    "        print(ori_index[i], vim_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
